{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "    div.text_cell_render, .CodeMirror pre, div.output {\n",
    "        font-size: 1.2em;\n",
    "        line-height: 1.2em;\n",
    "    }\n",
    "    .container {\n",
    "        width: 80%;\n",
    "    }\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Poshmark\n",
    "\n",
    "This notebook walks through how to scrape listings from [Poshmark.com](https://poshmark.com/). Poshmark is a social commerce platform where people buy and sell new and used clothing, shoes and accessories.\n",
    "\n",
    "![preview](images/poshmark-preview.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webpages 101\n",
    "\n",
    "Websites are built using HTML and CSS. HTML provides the layout for websites. CSS provides the styling like font sizes, colors and spacing.\n",
    "\n",
    "Scraping takes advantage of the inherent structure on webpages. We find data by using the repeating HTML elements and CSS classes on pages. \n",
    "\n",
    "CSS classes are repeatable styling given to components with similar styling. For example, the item cards on Poshmark, all have the same exact look and feel, so their HTML code looks rather similar.\n",
    "\n",
    "Check out: [Diesel Jeans](https://poshmark.com/brand/Diesel-Men-Jeans?sort_by=added_desc) and use the Google Chrome Inspector.\n",
    "<br><br>\n",
    "\n",
    "![Diesel Code](images/diesel-code.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the denim listings\n",
    "\n",
    "We'll use `requests` to pull down the website. Then, we'll print out the response variable which contains the HTML code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "\n",
    "url = \"https://poshmark.com/brand/Diesel-Men-Jeans?sort_by=added_desc\"\n",
    "response = get(url)\n",
    "print(response.text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `beautifulsoup` to parse the raw HTML. This is a package specially made for accessing HTML elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "type(html_soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use built-in methods to search for the repeating tiles in the markup. We take advantage of CSS classes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clothing_containers = html_soup.find_all('div', class_ = 'tile')\n",
    "print(type(clothing_containers))\n",
    "print(len(clothing_containers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_tile = clothing_containers[0]\n",
    "print(first_tile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BeautifulSoup` has a `prettify` method which allows use to make HTML code more readable with proper indentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_tile.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the values\n",
    "\n",
    "Now that we have the card isolated, we can extract the specific data points:\n",
    "- Item Title\n",
    "- Item Price\n",
    "- Item Size\n",
    "- Item Brand\n",
    "- Item Page Link\n",
    "- Item Image Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item Title\n",
    "first_title = first_tile.find('a', class_='tile__title')\n",
    "print(first_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_title = first_tile.find('a', class_='tile__title').get_text()\n",
    "print(first_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_title = first_tile.find('a', class_='tile__title').get_text(strip=True)\n",
    "print(first_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item Price\n",
    "first_price = first_tile.find('span', class_=\"fw--bold\")\n",
    "print(first_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_price = first_tile.find('span', class_=\"fw--bold\").get_text(strip=True)\n",
    "print(first_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item Size\n",
    "first_size = first_tile.find('a', class_=\"tile__details__pipe__size\").get_text(strip=True)\n",
    "print(first_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item Brand\n",
    "first_brand = first_tile.find('a', class_=\"tile__details__pipe__brand\").get_text(strip=True)\n",
    "print(first_brand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item Detail URL\n",
    "first_link = first_tile.find('a', class_='tile__title').get('href')\n",
    "print(first_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_link = 'http://www.poshmark.com' + first_tile.find('a', class_='tile__title').get('href')\n",
    "print(first_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item Image URL\n",
    "first_image = first_tile.find('img')\n",
    "print(first_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_image = first_tile.find('img').get('src')\n",
    "print(first_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format the data\n",
    "\n",
    "We scraped the raw data. However, all the data are strings. We need to convert them to the appropriate types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Title: ', first_title)\n",
    "print('Price: ', first_price)\n",
    "print('Size: ', first_size)\n",
    "print('Brand: ', first_brand)\n",
    "print('Link: ', first_link)\n",
    "print('Image: ', first_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The title and links can stay the same. However, we will need to convert price and size to `int`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_price = first_price.replace('$', '')\n",
    "print(type(fixed_price))\n",
    "print(fixed_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_price = int(first_price.replace('$', ''))\n",
    "print(type(fixed_price))\n",
    "print(fixed_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_size = int(first_size.replace('Size: ', ''))\n",
    "print(type(fixed_size))\n",
    "print(fixed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting a new feature\n",
    "\n",
    "If you took a close look at the image URL, you can see the path actually shows when the posting was created. Let's extract that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = first_image.find('2020')\n",
    "print(start_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_idx = start_idx + 10\n",
    "raw_date = first_image[start_idx:end_idx]\n",
    "print(raw_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "\n",
    "first_date = parse(raw_date)\n",
    "print(first_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the approximate days the item has been listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "diff = abs((first_date-now).days)\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Note:</b> In a professional workflow, you would create separate versions of the data.\n",
    "\n",
    "<br>\n",
    "\n",
    "<ol>\n",
    "    <li>Raw scraped data</li>\n",
    "    <li>Type formatted data</li>\n",
    "    <li>Data with new features</li>\n",
    "</ol>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 minute break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor code, create functions\n",
    "\n",
    "Here we'll refactor our code and create functions to extract all the data. A \"good\" function follows these guidelines:\n",
    "\n",
    "- Is sensibly named\n",
    "- Has a single responsibility\n",
    "- Includes a docstring\n",
    "- Returns a value\n",
    "- Is not longer than 50 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "from datetime import datetime\n",
    "\n",
    "def download_page(url):\n",
    "    \"Download HTML source for a given URL\"\n",
    "    response = get(url)\n",
    "    return response\n",
    "\n",
    "def create_soup(source):\n",
    "    \"Convert HTML source to BeautifulSoup object\"\n",
    "    soup = BeautifulSoup(source.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def extract_tiles(soup):\n",
    "    \"Extract all the clothing tile elements\"\n",
    "    containers = soup.find_all('div', class_ = 'tile')\n",
    "    return containers\n",
    "\n",
    "def extract_title(tile):\n",
    "    \"Extract the title string from a tile\"\n",
    "    title = tile.find('a', class_='tile__title').get_text(strip=True)\n",
    "    return title\n",
    "\n",
    "def extract_price(tile):\n",
    "    \"Extract the price integer from a tile\"\n",
    "    price_string = tile.find('span', class_=\"fw--bold\").get_text(strip=True)\n",
    "    price = int(price_string.replace('$', ''))\n",
    "    return price\n",
    "\n",
    "def extract_size(tile):\n",
    "    \"Extract the size integer from a tile\"\n",
    "    size_string = tile.find('a', class_=\"tile__details__pipe__size\").get_text(strip=True)\n",
    "    size = int(size_string.replace('Size: ', ''))\n",
    "    return size\n",
    "\n",
    "def extract_brand(tile):\n",
    "    \"Extract the brand string from a tile\"\n",
    "    brand = tile.find('a', class_=\"tile__details__pipe__brand\").get_text(strip=True)\n",
    "    return brand\n",
    "\n",
    "def extract_link(tile):\n",
    "    \"Extract the link string from a tile\"\n",
    "    partial_link = tile.find('a', class_='tile__title').get('href')\n",
    "    link = 'http://www.poshmark.com' + partial_link\n",
    "    return link\n",
    "\n",
    "def extract_image(tile):\n",
    "    \"Extract the image link string from a tile\"\n",
    "    image = tile.find('img').get('data-src')\n",
    "    return image\n",
    "    \n",
    "def extract_date(url):\n",
    "    \"Extract the posting date from a url\"\n",
    "    start = url.find('20')\n",
    "    end = start + 10\n",
    "    raw_date = url[start:end]\n",
    "    date = parse(raw_date)\n",
    "    return date\n",
    "\n",
    "def find_difference(date):\n",
    "    \"Find the amount of days an item has been listed\"\n",
    "    now = datetime.now()\n",
    "    diff = abs((date-now).days)\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_data(tile):\n",
    "    \"Run independent functions and return object of all values\"\n",
    "    try:\n",
    "        title = extract_title(tile)\n",
    "    except:\n",
    "        title = ''\n",
    "        \n",
    "    try:\n",
    "        price = extract_price(tile)\n",
    "    except: \n",
    "        price = ''\n",
    "        \n",
    "    try:\n",
    "        size = extract_size(tile)\n",
    "    except:\n",
    "        size = ''\n",
    "    \n",
    "    try:\n",
    "        brand = extract_brand(tile)\n",
    "    except:\n",
    "        brand = ''\n",
    "    \n",
    "    try: \n",
    "        link = extract_link(tile)\n",
    "    except:\n",
    "        link = ''\n",
    "        \n",
    "    try:\n",
    "        image = extract_image(tile)\n",
    "    except:\n",
    "        image = ''\n",
    "        \n",
    "    try:\n",
    "        date = extract_date(image)\n",
    "        difference = find_difference(date)\n",
    "    except:\n",
    "        date = ''\n",
    "        difference = ''\n",
    "        \n",
    "    return {\n",
    "        'title': title,\n",
    "        'price': price,\n",
    "        'size': size,\n",
    "        'brand': brand,\n",
    "        'link': link,\n",
    "        'image': image,\n",
    "        'date': date,\n",
    "        'difference': difference \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract all tiles on initial page\n",
    "\n",
    "Now we can use the function to extract all the data from the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://poshmark.com/brand/Naked_&_Famous_Denim-Men-Jeans\"\n",
    "\n",
    "page = download_page(url)\n",
    "soup_obj = create_soup(page)\n",
    "item_tiles = extract_tiles(soup_obj)\n",
    "item_objs = [combine_data(tile) for tile in item_tiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(item_objs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(item_objs[0])\n",
    "print('-------')\n",
    "print(item_objs[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(item_objs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Note:</b> There are hundreds of listings, but we can only scrape the first 48.\n",
    "\n",
    "<br>\n",
    "\n",
    "<p>Modern websites use JavaScript to load additional results to prevent long initial load times. Our initial page download only includes the first set of listings. If you want to extract all the listings, you will have to use a headless browser. It creates a Chrome/Firefox instance in the background to mimic a real page visit.</p>\n",
    "\n",
    "<br>\n",
    "\n",
    "<p>That is a bit more complicated and out of the scope for now. I can demonstrate in a future talk.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract other denim brands\n",
    "\n",
    "Let's extract the data for other denim brands as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands = ['J._Crew', 'Naked_&_Famous_Denim', \"Levi's\", 'Diesel']\n",
    "store = []\n",
    "\n",
    "for tag in brands:\n",
    "    url = f\"https://poshmark.com/brand/{tag}-Men-Jeans\"\n",
    "    page = download_page(url)\n",
    "    soup_obj = create_soup(page)\n",
    "    item_tiles = extract_tiles(soup_obj)\n",
    "    item_objs = [combine_data(tile) for tile in item_tiles]\n",
    "    store.append(item_objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(store))\n",
    "print(len(store[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands = ['J._Crew', 'Naked_&_Famous_Denim', \"Levi's\", 'Diesel']\n",
    "store = []\n",
    "\n",
    "for tag in brands:\n",
    "    url = f\"https://poshmark.com/brand/{tag}-Men-Jeans\"\n",
    "    page = download_page(url)\n",
    "    soup_obj = create_soup(page)\n",
    "    item_tiles = extract_tiles(soup_obj)\n",
    "    item_objs = [combine_data(tile) for tile in item_tiles]\n",
    "    store.extend(item_objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(store))\n",
    "print(len(store[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examing the scraped data\n",
    "\n",
    "You can bring the data into `pandas` for further examination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(store)\n",
    "print(df.info())\n",
    "print('')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'] = df['title'].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = df[['brand', 'price', 'size', 'difference', 'length']]\n",
    "numeric_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for extreme values\n",
    "numeric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare medians by brand\n",
    "numeric_df.groupby('brand')['price', 'difference', 'length'].median().reset_index().rename(\n",
    "    columns={'brand':'Brand', 'price':'Price', 'difference':'Days Listed', 'length':'Title Length'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "# df.to_csv('data/source_data.csv')\n",
    "# numeric_df.to_csv('data/numeric_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the distributions\n",
    "\n",
    "Use `matplotlib` to plot and analyze the distributions in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'].plot.hist(bins=12, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_keys = df['brand'].unique()\n",
    "for key in distinct_keys:\n",
    "    plt.figure();\n",
    "    df_subset = df[df.brand==key]\n",
    "    df_subset['price'].plot.hist(bins=12, alpha=0.2, title=key);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of days listed\n",
    "df['difference'].plot.hist(bins=12, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "distinct_keys = df['brand'].unique()\n",
    "for key in distinct_keys:\n",
    "    plt.figure();\n",
    "    df_subset = df[df.brand==key]\n",
    "    df_subset['difference'].plot.hist(bins=12, alpha=0.2, title=key);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of title length\n",
    "df['length'].plot.hist(bins=12, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions, Feedback and Ideas\n",
    "\n",
    "If you have any questions, please use the chat.\n",
    "\n",
    "If you would be so kind to [fill out this feedback form](https://tyshaikh.typeform.com/to/uHHKg1).\n",
    "\n",
    "If you have any ideas or requests, please share them.\n",
    "\n",
    "I'd like to run bi-weekly or even weekly demos like this for next 6 months. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
